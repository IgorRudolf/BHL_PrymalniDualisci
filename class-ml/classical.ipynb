{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ML**\n",
    "\n",
    "Here we perform classical Machine Learning models fitting, tunability and testing on test_dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/data/model_data/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All features are nummeric and there are no NAs. However for future use on other datasets, we will include imputer with median as imputed value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(\n",
    "    steps=[('imputer', SimpleImputer(strategy='median')),\n",
    "           ('scaler', StandardScaler())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will analyse 4 popular regression models:\n",
    "- Lasso\n",
    "- Ridge\n",
    "- KNN Regressor\n",
    "- XGB Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models \n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Pipeline(steps=[('preprocessor', pipe),('lasso', Lasso())])\n",
    "ridge = Pipeline(steps=[('preprocessor', pipe),('ridge', Ridge())])\n",
    "knn = Pipeline(steps=[('preprocessor', pipe),('knn', KNeighborsRegressor())])\n",
    "xgb = Pipeline(steps=[('preprocessor', pipe),('xgb', XGBRegressor())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we construct the Maximum Relevance Minimum Redundancy algorithm for regression problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def calculate_relevance_pearson(df, target):\n",
    "    y = df[target]\n",
    "    X = df.drop(columns=[target])\n",
    "    relevance = []\n",
    "\n",
    "    for feature in X.columns:\n",
    "        correlation, _ = pearsonr(X[feature], y)\n",
    "        relevance.append(abs(correlation))  \n",
    "    return np.array(relevance)\n",
    "\n",
    "def calculate_redundancy(feature1, feature2):\n",
    "    return abs(pearsonr(feature1, feature2)[0])\n",
    "\n",
    "def mrmr_feature_selection_regression(df, target, K, relevance_func):\n",
    "    features = df.columns.drop(target)\n",
    "    relevance_scores = relevance_func(df, target)\n",
    "    feature_relevance = dict(zip(features, relevance_scores))\n",
    "\n",
    "    selected_features = []\n",
    "    remaining_features = list(features)\n",
    "\n",
    "    first_feature = max(feature_relevance, key=feature_relevance.get)\n",
    "    selected_features.append(first_feature)\n",
    "    remaining_features.remove(first_feature)\n",
    "\n",
    "    while len(selected_features) < K:\n",
    "        scores = {}\n",
    "        for feature in remaining_features:\n",
    "            redundancy_sum = np.sum([\n",
    "                calculate_redundancy(df[feature], df[selected])\n",
    "                for selected in selected_features\n",
    "            ])\n",
    "            relevance = feature_relevance[feature]\n",
    "            redundancy = redundancy_sum / (len(selected_features) ** 2)\n",
    "            scores[feature] = relevance - redundancy\n",
    "\n",
    "        next_feature = max(scores, key=scores.get)\n",
    "        selected_features.append(next_feature)\n",
    "        remaining_features.remove(next_feature)\n",
    "\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pipe.fit(train.drop(columns=['Mean_Radiation']), train['Mean_Radiation'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "array must not contain infs or NaNs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m K \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 2\u001b[0m mrmr_feature_selection_regression(train, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean_Radiation\u001b[39m\u001b[38;5;124m'\u001b[39m, K, calculate_relevance_pearson)\n",
      "Cell \u001b[0;32mIn[16], line 19\u001b[0m, in \u001b[0;36mmrmr_feature_selection_regression\u001b[0;34m(df, target, K, relevance_func)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmrmr_feature_selection_regression\u001b[39m(df, target, K, relevance_func):\n\u001b[1;32m     18\u001b[0m     features \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mdrop(target)\n\u001b[0;32m---> 19\u001b[0m     relevance_scores \u001b[38;5;241m=\u001b[39m relevance_func(df, target)\n\u001b[1;32m     20\u001b[0m     feature_relevance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(features, relevance_scores))\n\u001b[1;32m     22\u001b[0m     selected_features \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[16], line 10\u001b[0m, in \u001b[0;36mcalculate_relevance_pearson\u001b[0;34m(df, target)\u001b[0m\n\u001b[1;32m      7\u001b[0m relevance \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m X\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m---> 10\u001b[0m     correlation, _ \u001b[38;5;241m=\u001b[39m pearsonr(X[feature], y)\n\u001b[1;32m     11\u001b[0m     relevance\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mabs\u001b[39m(correlation))  \n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(relevance)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/scipy/stats/_stats_py.py:4793\u001b[0m, in \u001b[0;36mpearsonr\u001b[0;34m(x, y, alternative, method)\u001b[0m\n\u001b[1;32m   4788\u001b[0m ym \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mastype(dtype) \u001b[38;5;241m-\u001b[39m ymean\n\u001b[1;32m   4790\u001b[0m \u001b[38;5;66;03m# Unlike np.linalg.norm or the expression sqrt((xm*xm).sum()),\u001b[39;00m\n\u001b[1;32m   4791\u001b[0m \u001b[38;5;66;03m# scipy.linalg.norm(xm) does not overflow if xm is, for example,\u001b[39;00m\n\u001b[1;32m   4792\u001b[0m \u001b[38;5;66;03m# [-5e210, 5e210, 3e200, -3e200]\u001b[39;00m\n\u001b[0;32m-> 4793\u001b[0m normxm \u001b[38;5;241m=\u001b[39m linalg\u001b[38;5;241m.\u001b[39mnorm(xm)\n\u001b[1;32m   4794\u001b[0m normym \u001b[38;5;241m=\u001b[39m linalg\u001b[38;5;241m.\u001b[39mnorm(ym)\n\u001b[1;32m   4796\u001b[0m threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-13\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/scipy/linalg/_misc.py:146\u001b[0m, in \u001b[0;36mnorm\u001b[0;34m(a, ord, axis, keepdims, check_finite)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# Differs from numpy only in non-finite handling and the use of blas.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_finite:\n\u001b[0;32m--> 146\u001b[0m     a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray_chkfinite(a)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(a)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/numpy/lib/function_base.py:630\u001b[0m, in \u001b[0;36masarray_chkfinite\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    628\u001b[0m a \u001b[38;5;241m=\u001b[39m asarray(a, dtype\u001b[38;5;241m=\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39morder)\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m a\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mchar \u001b[38;5;129;01min\u001b[39;00m typecodes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllFloat\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(a)\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m--> 630\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray must not contain infs or NaNs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[0;31mValueError\u001b[0m: array must not contain infs or NaNs"
     ]
    }
   ],
   "source": [
    "K = train.shape[1] - 1\n",
    "mrmr_feature_selection_regression(train, 'Mean_Radiation', K, calculate_relevance_pearson)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import uniform, loguniform, randint\n",
    "from skopt.space import Real, Integer, Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LASSO_PARAMS_R = {'lasso__alpha': loguniform(1e-5, 1e5),\n",
    "                  'lasso__max_iter': randint(1000,7000),\n",
    "                  'lasso__fit_intercept': [True, False],}\n",
    "RIDGE_PARAMS_R = {'ridge__alpha': loguniform(1e-5, 1e5),\n",
    "                    'ridge__max_iter': randint(1000,7000),\n",
    "                    'ridge__fit_intercept': [True, False],\n",
    "                    'ridge__solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']}\n",
    "KNN_PARAMS_R = {'knn__n_neighbors': randint(1, 100),\n",
    "                'knn__weights': ['uniform', 'distance'],\n",
    "                'knn__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n",
    "XGB_PARAMS_R = {'xgb__n_estimators': randint(100, 1000),\n",
    "                'xgb__max_depth': randint(3, 10),\n",
    "                'xgb__learning_rate': loguniform(0.01, 0.3),\n",
    "                'xgb__subsample': uniform(0.6, 0.4),\n",
    "                'xgb__colsample_bytree': uniform(0.6, 0.4),\n",
    "                'xgb__gamma': uniform(0, 0.5),\n",
    "                'xgb__reg_alpha': loguniform(1e-5, 1e5),\n",
    "                'xgb__reg_lambda': loguniform(1e-5, 1e5)}\n",
    "LASSO_PARAMS_B = {'lasso__alpha': Real(1e-5, 1e5, prior='log-uniform', transform='identity'),}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
